from gtts import gTTS
from moviepy.video.io.VideoFileClip import VideoFileClip
from moviepy.audio.io.AudioFileClip import AudioFileClip
from moviepy.video.VideoClip import ImageClip, TextClip
from moviepy.video.compositing.concatenate import concatenate_videoclips
from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip
# Remove the problematic imports and use only what's available
from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
import numpy as np
from pathlib import Path
from config. config import Config
import tempfile
import textwrap
from openai import OpenAI
import requests
import io
import re
import matplotlib.pyplot as plt
import matplotlib. patches as patches
import matplotlib
matplotlib.use('Agg')
import math

class VideoGenerator:
    """Generate cinematic teaching videos with animations, transitions, and diverse visuals"""
    
    def __init__(self):
        self.temp_dir = tempfile.mkdtemp()
        self.client = OpenAI(api_key=Config.OPENAI_API_KEY)
        self.lecturer_frames = []
        self.animation_frame_count = 0
    
    def generate_video(
        self,
        script: str,
        title: str,
        style: str = "simple_text",
        tts_service: str = "gtts",
        pdf_content: str = ""
    ) -> str:
        """
        Generate cinematic video from script with animations and transitions
        
        Args: 
            script:  Lecture script text
            title: Video title
            style: Visual style
            tts_service: TTS service to use
            pdf_content: Original PDF content for context
            
        Returns:
            Path to generated video file
        """
        try: 
            print("ðŸŽ¬ Starting cinematic video generation...")
            
            # Generate audio
            print("ðŸŽ¤ Generating audio narration...")
            audio_path = self._generate_audio(script, tts_service)
            
            # Generate animated lecturer
            print("ðŸ‘¨â€ðŸ« Creating animated virtual lecturer...")
            self._generate_animated_lecturer()
            
            # Analyze script and generate diverse visuals
            print("ðŸŽ¨ Generating diverse educational visuals...")
            visual_segments = self._analyze_and_generate_visuals(script, pdf_content)
            
            # Generate video clips with animations
            print("ðŸŽ¥ Creating animated video clips...")
            video_clips = self._generate_cinematic_visuals(
                script, title, visual_segments, audio_path
            )
            
            # Combine with transitions
            print("âœ¨ Adding cinematic transitions...")
            video_path = self._create_final_video_with_transitions(
                video_clips, audio_path, title
            )
            
            print("âœ… Cinematic video generation complete!")
            return video_path
            
        except Exception as e:
            raise Exception(f"Error generating video: {str(e)}")
    
    def _generate_audio(self, script: str, tts_service: str) -> str:
        """Generate audio from script using TTS"""
        try: 
            audio_path = Path(self.temp_dir) / "lecture_audio.mp3"
            
            if tts_service == "gtts": 
                tts = gTTS(text=script, lang='en', slow=False)
                tts.save(str(audio_path))
            else:
                tts = gTTS(text=script, lang='en', slow=False)
                tts.save(str(audio_path))
            
            return str(audio_path)
            
        except Exception as e:
            raise Exception(f"Error generating audio:  {str(e)}")
    
    def _apply_fade_effect(self, clip, fade_in_duration=0.5, fade_out_duration=0.5):
        """Apply custom fade in/out effect to clip"""
        def make_frame(t):
            frame = clip.get_frame(t)
            
            # Fade in
            if t < fade_in_duration: 
                alpha = t / fade_in_duration
                frame = (frame * alpha).astype('uint8')
            
            # Fade out
            elif t > clip.duration - fade_out_duration:
                alpha = (clip.duration - t) / fade_out_duration
                frame = (frame * alpha).astype('uint8')
            
            return frame
        
        from moviepy.video.VideoClip import VideoClip
        faded_clip = VideoClip(make_frame, duration=clip.duration)
        faded_clip. fps = clip.fps if hasattr(clip, 'fps') else 24
        
        return faded_clip
    
    def _generate_animated_lecturer(self):
        """Generate animated lecturer frames"""
        print("ðŸŽ­ Creating lecturer animation frames...")
        
        # Generate base lecturer image
        base_lecturer = self._generate_lecturer_avatar()
        
        # Create animation frames (breathing, gesturing)
        num_frames = 60  # 60 frames for smooth animation
        
        for i in range(num_frames):
            frame = self._animate_lecturer_frame(base_lecturer, i, num_frames)
            self.lecturer_frames.append(frame)
        
        print(f"âœ… Generated {len(self.lecturer_frames)} animation frames")
    
    def _generate_lecturer_avatar(self) -> Image. Image:
        """Generate AI lecturer avatar using DALL-E"""
        try:
            response = self.client.images.generate(
                model="dall-e-3",
                prompt="Professional university professor in business casual attire, friendly smile, teaching gesture with hand raised, neutral soft gradient background, photorealistic portrait, warm lighting, front-facing, upper body visible",
                size="1024x1024",
                quality="standard",
                n=1,
            )
            
            image_url = response.data[0].url
            img_response = requests.get(image_url)
            img = Image.open(io.BytesIO(img_response.content))
            img = img.resize((960, 1080), Image.Resampling.LANCZOS)
            
            print("âœ… Lecturer avatar generated")
            return img
            
        except Exception as e:
            print(f"âš ï¸ DALL-E unavailable, using illustrated lecturer:  {str(e)}")
            return self._create_illustrated_lecturer()
    
    def _create_illustrated_lecturer(self) -> Image.Image:
        """Create professional illustrated lecturer"""
        img = Image.new('RGB', (960, 1080), color=(240, 245, 255))
        draw = ImageDraw.Draw(img)
        
        # Gradient background
        for y in range(1080):
            color_value = int(240 + (y / 1080) * 15)
            draw.line([(0, y), (960, y)], fill=(color_value, color_value + 5, 255))
        
        # Head (circle)
        head_center = (480, 300)
        head_radius = 120
        draw.ellipse(
            [head_center[0] - head_radius, head_center[1] - head_radius,
             head_center[0] + head_radius, head_center[1] + head_radius],
            fill=(255, 220, 177), outline=(180, 140, 100), width=4
        )
        
        # Hair
        draw.arc(
            [head_center[0] - head_radius, head_center[1] - head_radius - 20,
             head_center[0] + head_radius, head_center[1]],
            start=0, end=180, fill=(80, 60, 40), width=25
        )
        
        # Eyes
        draw.ellipse([430, 270, 460, 295], fill=(50, 50, 50))
        draw.ellipse([500, 270, 530, 295], fill=(50, 50, 50))
        draw.ellipse([438, 278, 452, 287], fill=(255, 255, 255))
        draw.ellipse([508, 278, 522, 287], fill=(255, 255, 255))
        
        # Smile
        draw.arc([420, 290, 540, 360], start=0, end=180, fill=(200, 100, 100), width=4)
        
        # Neck
        draw.rectangle([450, 420, 510, 500], fill=(255, 210, 170))
        
        # Body (shirt)
        draw.polygon(
            [(300, 500), (660, 500), (700, 1080), (260, 1080)],
            fill=(50, 100, 180), outline=(30, 70, 150)
        )
        
        # Collar
        draw.polygon(
            [(450, 500), (480, 550), (480, 500)],
            fill=(255, 255, 255)
        )
        draw.polygon(
            [(510, 500), (480, 550), (480, 500)],
            fill=(255, 255, 255)
        )
        
        # Arms (gesture)
        # Left arm raised
        draw.ellipse([280, 650, 340, 710], fill=(255, 210, 170), outline=(200, 160, 130), width=3)
        draw.line([(320, 600), (310, 680)], fill=(50, 100, 180), width=45)
        
        # Right arm
        draw.ellipse([620, 750, 680, 810], fill=(255, 210, 170), outline=(200, 160, 130), width=3)
        draw.line([(650, 600), (650, 780)], fill=(50, 100, 180), width=45)
        
        # Add shadow for depth
        shadow = Image.new('RGBA', (960, 1080), (0, 0, 0, 0))
        shadow_draw = ImageDraw.Draw(shadow)
        shadow_draw. ellipse(
            [head_center[0] - head_radius + 5, head_center[1] - head_radius + 5,
             head_center[0] + head_radius + 5, head_center[1] + head_radius + 5],
            fill=(0, 0, 0, 30)
        )
        img.paste(shadow, (0, 0), shadow)
        
        return img
    
    def _animate_lecturer_frame(self, base_image:  Image.Image, frame_num: int, total_frames: int) -> Image.Image:
        """Create animated frame with subtle movements"""
        img = base_image.copy()
        
        # Breathing animation (subtle scale)
        scale_factor = 1.0 + 0.01 * math.sin(2 * math.pi * frame_num / total_frames)
        
        # Slight head movement
        x_offset = int(3 * math.sin(2 * math.pi * frame_num / total_frames))
        
        # Apply transformations
        width, height = img.size
        new_width = int(width * scale_factor)
        new_height = int(height * scale_factor)
        
        img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        # Crop back to original size
        left = (new_width - width) // 2
        top = (new_height - height) // 2
        img = img.crop((left, top, left + width, top + height))
        
        # Apply slight shift
        shifted_img = Image.new('RGB', (960, 1080), color=(240, 245, 255))
        shifted_img.paste(img, (x_offset, 0))
        
        return shifted_img
    
    def _analyze_and_generate_visuals(self, script: str, pdf_content: str) -> list:
        """Analyze script and generate diverse visual content"""
        segments = self._split_script_into_segments(script, 60)
        
        visual_segments = []
        
        for i, segment in enumerate(segments):
            print(f"ðŸ“Š Creating visual {i+1}/{len(segments)}...")
            
            content_type = self._detect_content_type(segment)
            
            # Generate appropriate visual based on type
            if content_type == "math":
                visual = self._generate_math_visual(segment)
            elif content_type == "diagram":
                visual = self._generate_flowchart(segment)
            elif content_type == "concept":
                visual = self._generate_concept_visual(segment)
            elif content_type == "list":
                visual = self._generate_list_visual(segment)
            elif content_type == "comparison":
                visual = self._generate_comparison_visual(segment)
            elif content_type == "timeline":
                visual = self._generate_timeline_visual(segment)
            else:
                visual = self._generate_modern_text_visual(segment)
            
            visual_segments.append({
                'text': segment,
                'visual': visual,
                'type':  content_type
            })
        
        return visual_segments
    
    def _detect_content_type(self, text: str) -> str:
        """Detect content type for appropriate visualization"""
        text_lower = text.lower()
        
        # Mathematical content
        if any(kw in text_lower for kw in ['equation', 'formula', 'calculate', 'derivative', 'integral', '=']):
            return "math"
        
        # List/enumeration
        if any(kw in text_lower for kw in ['first', 'second', 'third', 'steps', 'points', 'factors']):
            return "list"
        
        # Comparison
        if any(kw in text_lower for kw in ['versus', 'vs', 'compare', 'difference', 'contrast', 'while']):
            return "comparison"
        
        # Timeline/process
        if any(kw in text_lower for kw in ['then', 'next', 'after', 'before', 'timeline', 'history', 'evolution']):
            return "timeline"
        
        # Diagram/flowchart
        if any(kw in text_lower for kw in ['process', 'flow', 'step', 'procedure', 'algorithm']):
            return "diagram"
        
        # Conceptual
        if any(kw in text_lower for kw in ['concept', 'theory', 'principle', 'model', 'framework']):
            return "concept"
        
        return "text"
    
    # ... (Keep all the _generate_math_visual, _generate_flowchart, _generate_list_visual,
    #      _generate_comparison_visual, _generate_timeline_visual, _generate_concept_visual,
    #      _generate_modern_text_visual, _extract_math methods from the previous version)
    
    # I'll include them below for completeness: 
    
    def _generate_math_visual(self, text: str) -> Image.Image:
        """Generate mathematical visualization with equations"""
        fig, ax = plt.subplots(figsize=(9.6, 10.8), facecolor='#f8f9fa')
        ax.set_xlim(0, 10)
        ax.set_ylim(0, 10)
        ax.axis('off')
        
        ax.text(5, 9.5, 'Mathematical Concept', fontsize=32, ha='center', 
                weight='bold', color='#1e3a8a')
        
        math_expressions = self._extract_math(text)
        
        y_pos = 8
        for i, expr in enumerate(math_expressions[: 3]):
            rect = patches.FancyBboxPatch((1, y_pos - 0.6), 8, 1, 
                                          boxstyle="round,pad=0.1", 
                                          facecolor='#dbeafe', 
                                          edgecolor='#3b82f6', linewidth=3)
            ax.add_patch(rect)
            
            ax.text(5, y_pos, f"${expr}$", fontsize=28, ha='center', va='center',
                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            y_pos -= 2
        
        if 'x' in text. lower():
            ax_inset = fig.add_axes([0.15, 0.05, 0.7, 0.3])
            x = np.linspace(-5, 5, 100)
            y = x**2
            ax_inset.plot(x, y, 'b-', linewidth=3)
            ax_inset. grid(True, alpha=0.3)
            ax_inset. set_xlabel('x', fontsize=16)
            ax_inset. set_ylabel('y', fontsize=16)
            ax_inset.set_title('Visualization', fontsize=18)
        
        img_path = Path(self.temp_dir) / f"math_{hash(text)}.png"
        plt.tight_layout()
        plt.savefig(img_path, dpi=100, bbox_inches='tight', facecolor='#f8f9fa')
        plt.close()
        
        img = Image.open(img_path)
        img = img.resize((960, 1080), Image.Resampling.LANCZOS)
        return img
    
    def _generate_flowchart(self, text: str) -> Image.Image:
        """Generate flowchart/process diagram"""
        fig, ax = plt.subplots(figsize=(9.6, 10.8), facecolor='white')
        ax.set_xlim(0, 10)
        ax.set_ylim(0, 12)
        ax.axis('off')
        
        ax.text(5, 11, 'Process Flow', fontsize=32, ha='center', 
                weight='bold', color='#1e3a8a')
        
        sentences = text.split('.')[:4]
        steps = [s.strip() for s in sentences if s.strip()]
        
        y_start = 9
        box_height = 1.2
        spacing = 1.5
        
        for i, step in enumerate(steps):
            y_pos = y_start - i * (box_height + spacing)
            
            rect = patches.FancyBboxPatch(
                (1, y_pos - box_height/2), 8, box_height,
                boxstyle="round,pad=0.15",
                facecolor=['#dbeafe', '#fef3c7', '#d1fae5', '#fce7f3'][i % 4],
                edgecolor=['#3b82f6', '#f59e0b', '#10b981', '#ec4899'][i % 4],
                linewidth=3
            )
            ax.add_patch(rect)
            
            circle = patches.Circle((1.5, y_pos), 0.3, 
                                   facecolor=['#3b82f6', '#f59e0b', '#10b981', '#ec4899'][i % 4],
                                   edgecolor='white', linewidth=2)
            ax.add_patch(circle)
            ax.text(1.5, y_pos, str(i+1), fontsize=20, ha='center', va='center',
                   color='white', weight='bold')
            
            wrapped = textwrap.fill(step, width=40)
            ax.text(5, y_pos, wrapped, fontsize=14, ha='center', va='center',
                   wrap=True)
            
            if i < len(steps) - 1:
                ax.arrow(5, y_pos - box_height/2 - 0.2, 0, -0.8, 
                        head_width=0.4, head_length=0.3, fc='#6b7280', ec='#6b7280',
                        linewidth=2)
        
        img_path = Path(self.temp_dir) / f"flow_{hash(text)}.png"
        plt.tight_layout()
        plt.savefig(img_path, dpi=100, bbox_inches='tight')
        plt.close()
        
        img = Image.open(img_path)
        img = img.resize((960, 1080), Image.Resampling.LANCZOS)
        return img
    
    def _generate_list_visual(self, text: str) -> Image.Image:
        """Generate animated list/bullet points"""
        img = Image.new('RGB', (960, 1080), color=(248, 250, 252))
        draw = ImageDraw.Draw(img)
        
        try:
            title_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 48)
            body_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 32)
        except:
            title_font = ImageFont.load_default()
            body_font = ImageFont.load_default()
        
        draw.text((80, 80), "Key Points", fill=(30, 64, 175), font=title_font)
        draw.rectangle([80, 150, 400, 158], fill=(59, 130, 246))
        
        sentences = text.split('.')[:5]
        points = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
        
        colors = [(59, 130, 246), (16, 185, 129), (245, 158, 11), (236, 72, 153), (139, 92, 246)]
        y_offset = 220
        
        for i, point in enumerate(points):
            color = colors[i % len(colors)]
            
            draw.ellipse([80, y_offset, 120, y_offset + 40], fill=color)
            draw.text((100, y_offset + 20), str(i+1), fill=(255, 255, 255),
                     font=body_font, anchor="mm")
            
            wrapped = textwrap.fill(point, width=32)
            
            bbox = draw.textbbox((160, y_offset), wrapped, font=body_font)
            draw.rectangle([150, bbox[1] - 10, bbox[2] + 20, bbox[3] + 10],
                          fill=(255, 255, 255), outline=color, width=2)
            
            draw.text((160, y_offset), wrapped, fill=(30, 41, 59), font=body_font)
            
            y_offset += 160
        
        return img
    
    def _generate_comparison_visual(self, text: str) -> Image.Image:
        """Generate comparison/vs visualization"""
        img = Image.new('RGB', (960, 1080), color=(255, 255, 255))
        draw = ImageDraw.Draw(img)
        
        for y in range(1080):
            color = int(255 - (y / 1080) * 20)
            draw.line([(0, y), (960, y)], fill=(color, color, color + 20))
        
        try:
            title_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 44)
            body_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans. ttf", 28)
        except:
            title_font = ImageFont.load_default()
            body_font = ImageFont. load_default()
        
        draw.text((480, 60), "Comparison", fill=(30, 64, 175),
                 font=title_font, anchor="mm")
        
        draw.line([(480, 150), (480, 1020)], fill=(200, 200, 200), width=4)
        draw.text((480, 1000), "VS", fill=(200, 200, 200),
                 font=title_font, anchor="mm")
        
        draw.rectangle([40, 150, 460, 200], fill=(59, 130, 246))
        draw.text((250, 175), "Option A", fill=(255, 255, 255),
                 font=title_font, anchor="mm")
        
        draw.rectangle([500, 150, 920, 200], fill=(16, 185, 129))
        draw.text((710, 175), "Option B", fill=(255, 255, 255),
                 font=title_font, anchor="mm")
        
        sentences = text.split('.')[:6]
        
        for i in range(min(3, len(sentences))):
            y_pos = 280 + i * 180
            draw.ellipse([60, y_pos, 90, y_pos + 30], fill=(59, 130, 246))
            if i < len(sentences):
                wrapped = textwrap.fill(sentences[i]. strip(), width=18)
                draw.text((110, y_pos + 15), wrapped, fill=(30, 41, 59), font=body_font)
        
        for i in range(min(3, len(sentences) - 3)):
            y_pos = 280 + i * 180
            draw.ellipse([520, y_pos, 550, y_pos + 30], fill=(16, 185, 129))
            if i + 3 < len(sentences):
                wrapped = textwrap. fill(sentences[i + 3].strip(), width=18)
                draw.text((570, y_pos + 15), wrapped, fill=(30, 41, 59), font=body_font)
        
        return img
    
    def _generate_timeline_visual(self, text: str) -> Image.Image:
        """Generate timeline visualization"""
        fig, ax = plt.subplots(figsize=(9.6, 10.8), facecolor='white')
        ax.set_xlim(0, 10)
        ax.set_ylim(0, 10)
        ax.axis('off')
        
        ax.text(5, 9.5, 'Timeline', fontsize=36, ha='center',
               weight='bold', color='#1e3a8a')
        
        ax.plot([1, 9], [5, 5], 'k-', linewidth=4, color='#3b82f6')
        
        sentences = text.split('.')[:4]
        x_positions = np.linspace(1.5, 8.5, len(sentences))
        colors = ['#ef4444', '#f59e0b', '#10b981', '#8b5cf6']
        
        for i, (x, event) in enumerate(zip(x_positions, sentences)):
            circle = patches.Circle((x, 5), 0.3, facecolor=colors[i % 4],
                                   edgecolor='white', linewidth=3, zorder=10)
            ax.add_patch(circle)
            
            y_pos = 7 if i % 2 == 0 else 3
            
            rect = patches.FancyBboxPatch(
                (x - 0.8, y_pos - 0.5), 1.6, 1,
                boxstyle="round,pad=0.1",
                facecolor='white',
                edgecolor=colors[i % 4],
                linewidth=2
            )
            ax.add_patch(rect)
            
            ax. plot([x, x], [5.3 if y_pos > 5 else 4.7, y_pos - 0.5 if y_pos > 5 else y_pos + 0.5],
                   'k--', linewidth=1, color=colors[i % 4], alpha=0.5)
            
            wrapped = textwrap.fill(event. strip(), width=15)
            ax.text(x, y_pos, wrapped, fontsize=10, ha='center', va='center',
                   wrap=True)
        
        img_path = Path(self.temp_dir) / f"timeline_{hash(text)}.png"
        plt.tight_layout()
        plt.savefig(img_path, dpi=100, bbox_inches='tight')
        plt.close()
        
        img = Image.open(img_path)
        img = img.resize((960, 1080), Image.Resampling.LANCZOS)
        return img
    
    def _generate_concept_visual(self, text: str) -> Image.Image:
        """Generate concept visualization with AI"""
        try:
            words = text.split()[:50]
            concept_text = ' '.join(words)
            
            prompt = f"Educational illustration:  {concept_text}. Minimalist infographic style, clean design, soft colors, white background, easy to understand, vector art style."
            
            response = self.client.images.generate(
                model="dall-e-3",
                prompt=prompt,
                size="1024x1024",
                quality="standard",
                n=1,
            )
            
            image_url = response.data[0].url
            img_response = requests.get(image_url)
            img = Image.open(io.BytesIO(img_response.content))
            img = img.resize((960, 1080), Image.Resampling.LANCZOS)
            
            return img
            
        except Exception as e:
            print(f"âš ï¸ Concept visual generation failed: {str(e)}")
            return self._generate_modern_text_visual(text)
    
    def _generate_modern_text_visual(self, text: str) -> Image.Image:
        """Generate modern, clean text slide"""
        img = Image.new('RGB', (960, 1080), color=(255, 255, 255))
        
        overlay = Image.new('RGBA', (960, 1080), (0, 0, 0, 0))
        draw_overlay = ImageDraw.Draw(overlay)
        
        for y in range(1080):
            alpha = int(30 * (1 - y / 1080))
            draw_overlay.rectangle([(0, y), (960, y + 1)], fill=(59, 130, 246, alpha))
        
        img.paste(overlay, (0, 0), overlay)
        
        draw = ImageDraw.Draw(img)
        
        try: 
            title_font = ImageFont. truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold. ttf", 44)
            body_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans. ttf", 32)
        except:
            title_font = ImageFont.load_default()
            body_font = ImageFont. load_default()
        
        draw.rectangle([50, 80, 70, 150], fill=(59, 130, 246))
        draw.rectangle([50, 160, 70, 230], fill=(16, 185, 129))
        
        sentences = text.split('.')[:3]
        y_offset = 120
        
        for sentence in sentences:
            if sentence.strip():
                wrapped = textwrap.fill(sentence.strip(), width=35)
                draw.text((100, y_offset), wrapped, fill=(30, 41, 59), font=body_font)
                y_offset += 250
        
        return img
    
    def _extract_math(self, text: str) -> list:
        """Extract mathematical expressions"""
        patterns = [
            r'[a-zA-Z]\s*=\s*[^,. ]+',
            r'\d+\s*[+\-*/]\s*\d+\s*=\s*\d+',
            r'[a-zA-Z]\^[0-9]',
            r'\\frac\{[^}]+\}\{[^}]+\}',
        ]
        
        expressions = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            expressions.extend(matches)
        
        return expressions[: 3]
    
    def _generate_cinematic_visuals(self, script: str, title: str,
                                   visual_segments: list, audio_path: str) -> list:
        """Generate video clips with animations"""
        try:
            audio_clip = AudioFileClip(audio_path)
            duration = audio_clip.duration
            audio_clip. close()
            
            clips = []
            
            print("ðŸŽ¬ Creating opening title...")
            title_clip = self._create_cinematic_title(title, 4)
            # Apply custom fade effect
            title_clip = self._apply_fade_effect(title_clip, 0.5, 0.5)
            clips.append(title_clip)
            
            remaining_duration = duration - 4
            time_per_segment = remaining_duration / len(visual_segments) if visual_segments else remaining_duration
            
            for i, segment in enumerate(visual_segments):
                print(f"ðŸŽ¨ Creating segment {i+1}/{len(visual_segments)}...")
                
                segment_clip = self._create_animated_split_screen(
                    segment['visual'],
                    segment['text'],
                    time_per_segment,
                    i
                )
                
                # Apply fade transitions
                segment_clip = self._apply_fade_effect(segment_clip, 0.3, 0.3)
                
                clips.append(segment_clip)
            
            return clips
            
        except Exception as e:
            raise Exception(f"Error generating cinematic visuals: {str(e)}")
    
    def _create_animated_split_screen(self, content_visual: Image.Image,
                                     text:  str, duration: float, segment_index: int) -> ImageClip:
        """Create animated split-screen with lecturer"""
        # For simplicity, use single frame instead of animation
        # Get lecturer frame
        lecturer_frame_idx = (segment_index * 10) % len(self.lecturer_frames)
        lecturer_img = self.lecturer_frames[lecturer_frame_idx]
        
        # Create canvas
        canvas = Image.new('RGB', (1920, 1080), color=(255, 255, 255))
        
        # Paste lecturer (left)
        canvas.paste(lecturer_img, (0, 0))
        
        # Paste content (right)
        canvas.paste(content_visual, (960, 0))
        
        # Add divider with shadow
        draw = ImageDraw.Draw(canvas)
        draw.rectangle([957, 0, 963, 1080], fill=(200, 200, 200))
        draw.rectangle([963, 0, 965, 1080], fill=(150, 150, 150))
        
        # Convert to array and create clip
        img_array = np.array(canvas)
        clip = ImageClip(img_array).set_duration(duration)
        
        return clip
    
    def _create_cinematic_title(self, title: str, duration: float) -> ImageClip:
        """Create cinematic title"""
        img = Image.new('RGB', (1920, 1080), color=(15, 23, 42))
        draw = ImageDraw.Draw(img)
        
        # Gradient
        for y in range(1080):
            alpha = int(100 * math.sin(math.pi * y / 1080))
            color = (30 + alpha // 2, 64 + alpha // 3, 175 + alpha // 5)
            draw.line([(0, y), (1920, y)], fill=color)
        
        # Decorative circles
        for i in range(20):
            x = (i * 100 + 50) % 1920
            y = (i * 80 + 100) % 1080
            size = 30 + (i * 10) % 50
            alpha = 150 - (i * 5)
            
            circle = Image.new('RGBA', (1920, 1080), (0, 0, 0, 0))
            circle_draw = ImageDraw.Draw(circle)
            circle_draw. ellipse([x, y, x + size, y + size],
                              fill=(59, 130, 246, alpha))
            img.paste(circle, (0, 0), circle)
        
        try:
            title_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 90)
            subtitle_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 44)
        except:
            title_font = ImageFont.load_default()
            subtitle_font = ImageFont. load_default()
        
        wrapped_title = textwrap.fill(title, width=30)
        
        # Shadow
        draw.text((962, 442), wrapped_title, fill=(0, 0, 0), font=title_font, anchor="mm")
        # Main text
        draw.text((960, 440), wrapped_title, fill=(255, 255, 255), font=title_font, anchor="mm")
        
        # Subtitle
        subtitle = "ðŸŽ“ AI-Powered Educational Lecture"
        draw.text((960, 580), subtitle, fill=(147, 197, 253), font=subtitle_font, anchor="mm")
        
        # Bottom line
        draw.rectangle([760, 650, 1160, 658], fill=(59, 130, 246))  
        
        img_array = np.array(img)
        clip = ImageClip(img_array).set_duration(duration)
        
        return clip
    
    def _split_script_into_segments(self, script:  str, target_duration: float) -> list:
        """Split script intelligently"""
        words_per_second = 2.5
        target_words = int(target_duration * words_per_second)
        
        paragraphs = script.split('\n\n')
        
        if not paragraphs or len(paragraphs) < 2:
            sentences = script.split('. ')
            segments = []
            current = []
            
            for sentence in sentences:
                current.append(sentence)
                if len(' '.join(current).split()) >= target_words:
                    segments.append('. '.join(current) + '.')
                    current = []
            
            if current: 
                segments.append('. '. join(current) + '.')
            
            return segments if segments else [script]
        
        return paragraphs
    
    def _create_final_video_with_transitions(self, video_clips: list,
                                            audio_path: str, title:  str) -> str:
        """Combine clips with smooth transitions"""
        try:
            print("ðŸŽ¬ Applying transitions and compositing...")
            
            final_video = concatenate_videoclips(video_clips, method="compose")
            
            audio = AudioFileClip(audio_path)
            final_video = final_video.set_audio(audio)
            
            Config.VIDEO_DIR. mkdir(parents=True, exist_ok=True)
            
            output_path = Path(Config.VIDEO_DIR) / f"{title}_lecture.mp4"
            
            print(f"ðŸ’¾ Rendering final video to:  {output_path}")
            
            final_video.write_videofile(
                str(output_path),
                fps=24,
                codec='libx264',
                audio_codec='aac',
                temp_audiofile=str(Path(self.temp_dir) / 'temp-audio.m4a'),
                remove_temp=True,
                logger=None,
                preset='medium',
                threads=4,
                bitrate='5000k'
            )
            
            final_video.close()
            audio.close()
            for clip in video_clips:
                clip.close()
            
            print(f"âœ… Video saved successfully: {output_path}")
            
            return str(output_path)
            
        except Exception as e:
            raise Exception(f"Error creating final video: {str(e)}")